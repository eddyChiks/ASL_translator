{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e41cf74e-423a-4821-9fc6-95d1f95688b6",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175fb1b1-52e4-4408-95f1-0997a1fe88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from torch.utils.data import DataLoader,TensorDataset, SubsetRandomSampler, ConcatDataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "labels = {0:'A', 1:'B', 2:'C',3:'D', 4:'del',5:'E', 6:'F', 7:'G', 8:'H', 9:'I', 10:'J', 11:'K', 12:'L', 13:'M', 14:'N',15:'O', 16:'P', 17:'Q', 18:'R', 19:'S', 20:'space',21:'T', 22:'U', 23:'V', 24:'W', 25:'X', 26:'Y', 27:'Z'}\n",
    "#Hand detection module\n",
    "cap = cv2.VideoCapture(0)\n",
    "mpHands = mp.solutions.hands\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_model = mpHands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "\n",
    "def get_data(folder):\n",
    "    x = torch.empty((8400,3,75,75),dtype=torch.float32)\n",
    "    y = []\n",
    "    transforms = ToTensor()\n",
    "    listdir = os.listdir(folder)\n",
    "    for i in range(len(listdir)):\n",
    "        sublistdir = os.listdir(os.path.join(folder,listdir[i]))\n",
    "        for j in range (300) :\n",
    "            path = os.path.join(folder,listdir[i],sublistdir[j])\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img,(75,75))\n",
    "            #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = transforms(img)\n",
    "            x[(i*300)+j] = img\n",
    "            values = list(labels.values())\n",
    "            keys = list(labels.keys())\n",
    "            y.append(keys[values.index(listdir[i])])\n",
    "    y = torch.LongTensor(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29749dad-48bd-4f3d-aa86-cf0ead36cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(image,landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "    if x-10>0 and y-10>0 and x+w+10<image.shape[1] and y+h+10<image.shape[0]:\n",
    "        return [x-10, y-10, x + w + 10, y + h +10]\n",
    "    else:\n",
    "        if x-10<=0:\n",
    "            x-=x-10\n",
    "        if y-10<=0:\n",
    "            y-=y-10\n",
    "        if x+w+10>=image.shape[1]:\n",
    "            w-= x+w+10-image.shape[1]\n",
    "        if y+h+10>=image.shape[0]:\n",
    "            h-=y+h+10-image.shape[0]\n",
    "        return [x-10, y-10, x + w + 10, y + h +10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97628a2-bfec-434d-ab9d-48f803edde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_data('data_processed')\n",
    "dataset = TensorDataset(x,y)\n",
    "train_set, test_set = random_split(dataset, [6800,1600])\n",
    "train_dataloader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "test_dataloader = DataLoader(test_set,batch_size=32,shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed957ac7-c36a-4e94-91eb-d4024c2b98e2",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1974ed9-aa71-4852-a802-9d6083c9d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of CNN\n",
    "class myCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,3)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,15,3)\n",
    "        self.fc1 = nn.Linear(15*34*34,100)\n",
    "        self.fc2 = nn.Linear(100,60)\n",
    "        self.fc3 = nn.Linear(60,28)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # first convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x=self.pool(x)\n",
    "\n",
    "        # second convolution\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "                \n",
    "        # fully connected\n",
    "        x = torch.flatten(x,1) # flatten all dimensions except the batch\n",
    "\n",
    "        # fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # fc2\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # fc out\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2c64da-b0b2-41df-81bc-084a72129d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiation of model\n",
    "model = myCNN().to(device)\n",
    "#initialization of learning parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b9ec88-49d4-4cb5-a4c9-2eee60dee975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingLoop(train_dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        # move data on gpu\n",
    "        X = X.to(device)\n",
    "        X=X.float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        # backpropagation \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss = loss.item()\n",
    "            print(f\"The loss is {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26159a6e-1045-4f4b-8ab8-9c0d54572198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingLoop(test_dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    print_size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X,y in test_dataloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            X=X.float()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    test_loss = test_loss/num_batches\n",
    "    correct = correct / print_size\n",
    "\n",
    "    print(f\"Testing accuracy: {correct * 100}, Average loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8606d-a36e-4abf-aaa3-dcb780dbb2ea",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793e791-1807-4cd9-adf3-5c2e1cb1b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    trainingLoop(train_dataloader,model,loss_fn,optimizer)\n",
    "    testingLoop(test_dataloader,model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0bd4ec-f62d-435f-979b-549a67a7d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'ASL_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
